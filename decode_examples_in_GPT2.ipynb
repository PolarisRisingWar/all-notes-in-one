{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_2_english_path = \"/data/pretrained_models/gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(gpt_2_english_path)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(gpt_2_english_path, pad_token_id=tokenizer.eos_token_id)\n",
    "#在这里手动设置pad_token_id是为了取消warning。也可以通过在`generate()`函数中添加入参`pad_token_id=50256`或`pad_token_id=model.config.eos_token_id`实现相同的目标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = tokenizer(\"I want to say\", padding=False, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I want to say that I\\'m not a fan of the idea of a \"big-budget\" movie. I\\'m not a fan of the idea of a \"big-budget\" movie. I\\'m not a fan of the idea of a \"'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_output = model.generate(**toks, max_length=50)\n",
    "tokenizer.decode(greedy_output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I want to say thank you to all of you who have supported me over the years. I want to say thank you to all of you who have supported me over the years. I want to say thank you to all of you who have supported me'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_output = model.generate(\n",
    "    **toks, \n",
    "    max_length=50, \n",
    "    num_beams=5, \n",
    "    early_stopping=True\n",
    ")\n",
    "tokenizer.decode(beam_output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I want to say thank you to all of you who have supported me over the years. I am so grateful for all the support you have given me, and I hope you will continue to do the same.\\n\\nThank you all for your support'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_output = model.generate(\n",
    "    **toks, \n",
    "    max_length=50, \n",
    "    num_beams=5, \n",
    "    early_stopping=True,\n",
    "    no_repeat_ngram_size=2\n",
    ")\n",
    "tokenizer.decode(beam_output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: I want to say thank you to all of you who have supported me over the years. I am so grateful for all the support you have given me, and I hope you will continue to do the same.\n",
      "\n",
      "Thank you all for your support\n",
      "1: I want to say thank you to all of you who have supported me over the years. I am so grateful for all the support you have given me, and I hope you will continue to do the same.\n",
      "\n",
      "Thank you for your support.\n",
      "2: I want to say thank you to all of you who have supported me over the years. I am so grateful for all the support you have given me, and I hope you will continue to do the same.\n",
      "\n",
      "Thank you for your continued support\n",
      "3: I want to say thank you to all of you who have supported me over the years. I am so grateful for all the support you have given me, and I hope you will continue to do the same. Thank you.\n",
      "\n",
      "Thank you for\n",
      "4: I want to say thank you to all of you who have supported me over the years. I am so grateful for all the support you have given me, and I hope you will continue to do the same.\n",
      "\n",
      "Thank you for your support,\n"
     ]
    }
   ],
   "source": [
    "beam_outputs = model.generate(\n",
    "    **toks, \n",
    "    max_length=50, \n",
    "    num_beams=5, \n",
    "    early_stopping=True,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=5\n",
    ")\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to say kid this — I am talking about you, right? So, yeah — Tappy Charlie? Help me with this work! We're going to get a place in your joint. We're going to get a place.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.random.manual_seed(20240315)\n",
    "\n",
    "sample_output = model.generate(\n",
    "    **toks, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to say I would be a good president if I could.\"\n",
      "\n",
      "Follow @politico\n"
     ]
    }
   ],
   "source": [
    "sample_output = model.generate(\n",
    "    **toks, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=0,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to say 'Thank you for listening to me talk,'\" he says. \"I was really pleased that I received the letter.\"\n"
     ]
    }
   ],
   "source": [
    "sample_output = model.generate(\n",
    "    **toks, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to say how honored I am to attend your farewell lecture, honor some of my colleagues who will come to Earth in the next few years to lead the next phase of our strategic work here at our company. And do not hesitate to come here\n"
     ]
    }
   ],
   "source": [
    "sample_output = model.generate(\n",
    "    **toks, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_p=0.92, \n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I want to say my name to you! I'm sure you already know what it is, but I can't help but hear the same from all of you. I wanted to speak to you about my family in particular. I know it is just\n",
      "1: I want to say, if you haven't been paying attention, I should ask you this:\n",
      "\n",
      "Did you see this? The following image of this image (a post titled The War of the World) shows how China is now fighting an armed\n",
      "2: I want to say it's something that has had a huge impact on the football team for me as a kid, so I'll take it as a sign of respect for our coaches.\n",
      "\n",
      "\"Obviously I've always loved this game. I've\n"
     ]
    }
   ],
   "source": [
    "sample_outputs = model.generate(\n",
    "    **toks,\n",
    "    do_sample=True,\n",
    "    max_length=50,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=3\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envlegal1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
