{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3efc2e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#导包\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "from torch_geometric.nn.conv import APPNP\n",
    "from torch_geometric.nn.models import CorrectAndSmooth\n",
    "\n",
    "import torch_geometric.utils as pyg_utils\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e108cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#设置泛用初始超参数\n",
    "hp={'device':torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    'num_of_layers':2,\n",
    "    'hidden_units':64,\n",
    "   'dropout_rate':0.5,\n",
    "   'l2_r':0.005,\n",
    "   'learning_rate':0.1,\n",
    "   'early_stopping':100,\n",
    "   'epoch':50,\n",
    "   'batch_size':10}  #部分抄自PPNP论文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faea3eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#设置模型与model-specific超参数\n",
    "#APPNP超参\n",
    "hp_appnp={'alpha':0.1,\n",
    "         'K':10}\n",
    "\n",
    "#C&S超参\n",
    "hp_cs={'correct_layer':50,\n",
    "       'correct_alpha':1,\n",
    "       'smooth_layer':50,\n",
    "       'smooth_alpha':0.8,\n",
    "       'autoscale':False,\n",
    "      'scale':20}\n",
    "#从https://github.com/rusty1s/pytorch_geometric/blob/master/examples/correct_and_smooth.py抄的超参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c7a614b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class APPNP_self1(torch.nn.Module):\n",
    "    #参考PyG设置的参数什么的\n",
    "    #就PyG设置的就是没有predict部分，所以我也把predict部分放在GNNStack里面了\n",
    "    #就我想了一下，我觉得用MessagePassing类不方便，就还是用torch的Module类了\n",
    "    \n",
    "    def __init__(self,K,alpha):\n",
    "        #别的参数暂时省略\n",
    "        super(APPNP_self1,self).__init__()\n",
    "        self.K=K\n",
    "        self.alpha=alpha\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        #首先尝试使用dense_tensor，如果不行再转sparse_tensor\n",
    "        (row, col)= edge_index\n",
    "        node_num=max(row.max(),col.max())+1\n",
    "        adj = torch.zeros((node_num,node_num))\n",
    "        adj=adj.to(hp['device'])\n",
    "        adj[row, col] = torch.ones(row.numel()).to(hp['device'])\n",
    "        \n",
    "        self_loop=torch.eye(adj.size()[0]).to(hp['device'])  #自环\n",
    "        adj=adj+self_loop  #\\slide{A}\n",
    "        degree_vector=torch.sum(adj,dim=1).cpu()  #度矩阵\n",
    "        degree_vector=1/np.sqrt(degree_vector)  #D-1/2\n",
    "        degree_matrix=torch.diag(degree_vector).to(hp['device'])\n",
    "        adj=torch.mm(degree_matrix,adj)\n",
    "        adj=torch.mm(adj,degree_matrix)  #\\hat{\\slide{A}}\n",
    "        \n",
    "        H=x.clone()\n",
    "        Z=x.clone()\n",
    "        \n",
    "        for k in range(self.K-1):\n",
    "            Z=torch.mm(adj,Z)\n",
    "            Z=Z*(1-self.alpha)\n",
    "            Z=Z+self.alpha*H\n",
    "        \n",
    "        Z=torch.mm(adj,Z)\n",
    "        Z=Z*(1-self.alpha)\n",
    "        Z=Z+self.alpha*H\n",
    "        Z=F.log_softmax(Z, dim=1)\n",
    "        \n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06f00eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_index2sparse_tensor(edge_index,node_num):\n",
    "    sizes=(node_num,node_num)\n",
    "    v=torch.ones(edge_index[0].numel()).to(hp['device'])  #边数\n",
    "    return torch.sparse_coo_tensor(edge_index, v, sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8ec1e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class APPNP_self2(torch.nn.Module):   \n",
    "    #dense tensor在GPU上OOM了，我滚过来写稀疏矩阵了\n",
    "    def __init__(self,K,alpha):\n",
    "        #别的参数暂时省略\n",
    "        super(APPNP_self2,self).__init__()\n",
    "        self.K=K\n",
    "        self.alpha=alpha\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        node_num=x.size()[0]\n",
    "        edge_index, _ = pyg_utils.add_self_loops(edge_index,num_nodes=node_num)  #添加自环（\\slide{A}）\n",
    "        adj=edge_index2sparse_tensor(edge_index,node_num)  #将\\slide{A}转换为稀疏矩阵\n",
    "        degree_vector=torch.sparse.sum(adj,0)  #度数向量\n",
    "        degree_vector=degree_vector.to_dense().cpu()\n",
    "        degree_vector=1/np.sqrt(degree_vector)\n",
    "        degree_matrix=torch.diag(degree_vector).to(hp['device'])\n",
    "        adj=torch.sparse.mm(adj.t(),degree_matrix.t())\n",
    "        adj=adj.t()\n",
    "        adj=torch.mm(adj,degree_matrix)\n",
    "        adj=adj.to_sparse()\n",
    "        \n",
    "        H=x.clone()\n",
    "        \n",
    "        for k in range(self.K-1):\n",
    "            x=torch.mm(adj,x)\n",
    "            x=x*(1-self.alpha)\n",
    "            x=x+self.alpha*H\n",
    "        \n",
    "        x=torch.mm(adj,x)\n",
    "        x=x*(1-self.alpha)\n",
    "        x=x+self.alpha*H\n",
    "        x=F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d9cf17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入数据集\n",
    "#为了设置随机切分，所以不使用默认的public切分法（但是我最后也没有用到随机切分……算了，就这样吧）\n",
    "#PPNP的论文切分法看起来好复杂，算了\n",
    "\n",
    "#dataset_root&name_map\n",
    "ds_rn_map=[('/tmp/cora','Cora'),('./tmp/citeseer','CiteSeer'),('./tmp/pubmed','PubMed')]\n",
    "#使用示例代码：dataset1 = Planetoid(root=ds_rn_map[1][0], name=ds_rn_map[1][1],split='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "836d80cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNStack(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, args_general, args_special):\n",
    "        super(GNNStack, self).__init__()\n",
    "        \n",
    "        self.lin1=nn.Linear(input_dim,args_general['hidden_units'])\n",
    "        self.appnp=APPNP_self2(args_special['K'],args_special['alpha'])  #APPNP vs. APPNP_self2\n",
    "        self.lin2=nn.Linear(args_general['hidden_units'],output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "          \n",
    "        x=self.lin1(x)\n",
    "        x=self.appnp(x,edge_index)\n",
    "        x=self.lin2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "        #总之是softmax→取对数，具体细节还没看\n",
    "        #softmax文档里说如果损失函数用nll_loss的话就得用这个而不是softmax\n",
    "\n",
    "    def loss(self, pred, label):\n",
    "        return F.nll_loss(pred, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "367d2f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,loader,is_val):\n",
    "    model.eval()\n",
    "    total=0\n",
    "    correct=0\n",
    "    for batch in loader:\n",
    "        batch.to(hp['device'])\n",
    "        with torch.no_grad():\n",
    "            pred = model(batch).max(dim=1)[1]\n",
    "            label = batch.y\n",
    "\n",
    "        mask = batch.val_mask if is_val else batch.test_mask\n",
    "        # node classification: only evaluate on nodes in test set\n",
    "        pred = pred[mask]\n",
    "        label = batch.y[mask]\n",
    "        total += torch.sum(mask).item()\n",
    "            \n",
    "        correct += pred.eq(label).sum().item()\n",
    "        \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "995578c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 9s, sys: 3min 32s, total: 6min 41s\n",
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_accs=[]\n",
    "best_models=[]\n",
    "\n",
    "#以下训练集、验证集、测试集的切分有点问题，日志应该记录验证集上的结果，早停也应该由验证集来。\n",
    "#最后应该输出在验证集上最好模型在测试集上的结果？\n",
    "\n",
    "#呃，dropout也没有加，GNNStack也没有叠层，也没有加BN什么的……也没有调参，epoch也只有50轮……\n",
    "#凑合过吧\n",
    "\n",
    "#还有这个DataLoader其实我也不知道需不需要，如果可以的话整张图塞进去train也行吧\n",
    "\n",
    "for i in range(3):\n",
    "    dataset=Planetoid(root=ds_rn_map[i][0], name=ds_rn_map[i][1],split='random')\n",
    "    loader = DataLoader(dataset, batch_size=hp['batch_size'], shuffle=True)\n",
    "    input_dim=dataset.num_features\n",
    "    output_dim=dataset.num_classes\n",
    "    model=GNNStack(input_dim,output_dim,hp,hp_appnp)\n",
    "    model.to(hp['device'])\n",
    "    best_models.append(model)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=hp['learning_rate'])\n",
    "    this_test_accs=[]\n",
    "    before_biggest_test_acc=0\n",
    "    count_early_stopping=0\n",
    "    for epoch in range(hp['epoch']):\n",
    "        model.train()\n",
    "        for batch in loader:\n",
    "            batch=batch.to(hp['device'])\n",
    "            optimizer.zero_grad()\n",
    "            out=model(batch)\n",
    "            labels=batch.y\n",
    "            loss=model.loss(out[batch.train_mask],labels[batch.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if epoch%10==0:\n",
    "            test_acc=test(model,loader,True)\n",
    "            if test_acc>before_biggest_test_acc:\n",
    "                before_biggest_test_acc=test_acc\n",
    "                best_models[i]=model.state_dict()\n",
    "                count_early_stopping=0\n",
    "            else:\n",
    "                count_early_stopping+=1\n",
    "                if count_early_stopping>hp['early_stopping']:\n",
    "                    break\n",
    "            this_test_accs.append(test_acc)\n",
    "    \n",
    "    test_accs.append(this_test_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6b535ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.08, 0.12, 0.556, 0.728, 0.73],\n",
       " [0.228, 0.184, 0.522, 0.556, 0.608],\n",
       " [0.396, 0.514, 0.222, 0.514, 0.562]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1078f705",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNS_self(torch.nn.Module):\n",
    "    #参数照抄PyG了，意思也一样\n",
    "    def __init__(self,correct_layer,correct_alpha,smooth_layer,smooth_alpha,autoscale,scale):\n",
    "        super(CNS_self,self).__init__()\n",
    "        self.correct_layer=correct_layer\n",
    "        self.correct_alpha=correct_alpha\n",
    "        self.smooth_layer=smooth_layer\n",
    "        self.smooth_alpha=smooth_alpha\n",
    "        self.autoscale=autoscale\n",
    "        self.scale=scale\n",
    "    \n",
    "    def correct(self,Z,Y,mask,edge_index):\n",
    "        \"\"\"\n",
    "        Z:base prediction\n",
    "        Y:true label（第一维尺寸是训练集节点数）\n",
    "        mask:训练集mask\n",
    "        \"\"\"\n",
    "        #将Y扩展为独热编码矩阵\n",
    "        Y=F.one_hot(Y)\n",
    "        \n",
    "        num_nodes=Z.size()[0]\n",
    "        num_features=Z.size()[1]\n",
    "        E=torch.zeros(num_nodes,num_features).to(hp['device'])\n",
    "        E[mask]=Z[mask]-Y\n",
    "        \n",
    "        edge_index, _ = pyg_utils.add_self_loops(edge_index,num_nodes=num_nodes)  #添加自环（\\slide{A}）\n",
    "        adj=edge_index2sparse_tensor(edge_index,num_nodes)\n",
    "        degree_vector=torch.sparse.sum(adj,0)  #度数向量\n",
    "        degree_vector=degree_vector.to_dense().cpu()\n",
    "        degree_vector=np.power(degree_vector,-0.5)\n",
    "        degree_matrix=torch.diag(degree_vector).to(hp['device'])\n",
    "        adj=torch.sparse.mm(adj.t(),degree_matrix.t())\n",
    "        adj=adj.t()\n",
    "        adj=torch.mm(adj,degree_matrix)\n",
    "        adj=adj.to_sparse()\n",
    "        \n",
    "        x=E.clone()\n",
    "        if self.autoscale==True:\n",
    "            for k in range(self.correct_layer):\n",
    "                x=torch.sparse.mm(adj,x)\n",
    "                x=x*self.correct_alpha\n",
    "                x=x+(1-self.correct_alpha)*E\n",
    "            sigma=1/(mask.sum().item())*(E.sum())\n",
    "            Z=Z-x\n",
    "            Z[~mask]=Z[~mask]-sigma*F.softmax(x[~mask],dim=1)\n",
    "        else:\n",
    "            for k in range(self.correct_layer):\n",
    "                x=torch.sparse.mm(adj,x)\n",
    "                x=x*self.correct_alpha\n",
    "                x=x+(1-self.correct_alpha)*E\n",
    "                x[mask]=E[mask]\n",
    "            Z=Z-self.scale*x        \n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def smooth(self,Z,Y,mask,edge_index):\n",
    "        #将Y扩展为独热编码矩阵\n",
    "        Y=F.one_hot(Y)\n",
    "        \n",
    "        num_nodes=Z.size()[0]\n",
    "        G=Z.clone()\n",
    "        G[mask]=Y.float()\n",
    "        \n",
    "        edge_index, _ = pyg_utils.add_self_loops(edge_index,num_nodes=num_nodes)  #添加自环（\\slide{A}）\n",
    "        adj=edge_index2sparse_tensor(edge_index,num_nodes)\n",
    "        degree_vector=torch.sparse.sum(adj,0)  #度数向量\n",
    "        degree_vector=degree_vector.to_dense().cpu()\n",
    "        degree_vector=np.power(degree_vector,-0.5)\n",
    "        degree_matrix=torch.diag(degree_vector).to(hp['device'])\n",
    "        adj=torch.sparse.mm(adj.t(),degree_matrix.t())\n",
    "        adj=adj.t()\n",
    "        adj=torch.mm(adj,degree_matrix)\n",
    "        adj=adj.to_sparse()  #adj就是S\n",
    "        \n",
    "        x=G.clone()\n",
    "        for k in range(self.smooth_layer):\n",
    "            x=torch.sparse.mm(adj,x)\n",
    "            x=x*self.smooth_alpha\n",
    "            x=x+(1-self.smooth_alpha)*G\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b36568f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在数据集Cora中，测试集上的原始accuracy是0.122\n",
      "Correct and smooth...\n",
      "在数据集Cora中，测试集上的最终accuracy是0.68\n",
      "\n",
      "在数据集CiteSeer中，测试集上的原始accuracy是0.19\n",
      "Correct and smooth...\n",
      "在数据集CiteSeer中，测试集上的最终accuracy是0.483\n",
      "\n",
      "在数据集PubMed中，测试集上的原始accuracy是0.402\n",
      "Correct and smooth...\n",
      "在数据集PubMed中，测试集上的最终accuracy是0.715\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#应用跑了APPNP的best_model跑C&S，或者纯线性模型跑C&S\n",
    "#C结果：纯线性模型会有很高的结果提升，APPNP的话就妹有\n",
    "\n",
    "#就是……因为PyG的C&S是只分了训练集/测试集，所以我就直接这么干了：训练集上训练，测试集上输出\n",
    "#（毕竟没有选择模型参数的过程嘛，所以验证集好像没用的样子）\n",
    "\n",
    "#这个数据集是有问题的，跟前面训练模型时候的数据集应该是同一种split方法，要不然不河狸。但是我懒得改了，就这样吧\n",
    "\n",
    "#参考：https://github.com/rusty1s/pytorch_geometric/blob/master/examples/correct_and_smooth.py\n",
    "#原代码里面那个DAD和DA又是什么东西啊？\n",
    "\n",
    "post = CNS_self(hp_cs['correct_layer'],hp_cs['correct_alpha'],hp_cs['smooth_layer'],hp_cs['smooth_alpha'],\n",
    "                        hp_cs['autoscale'],hp_cs['scale'])  #CorrectAndSmooth vs. CNS_self\n",
    "\n",
    "for i in range(3):\n",
    "    dataset=Planetoid(root=ds_rn_map[i][0], name=ds_rn_map[i][1],split='random')\n",
    "    data=dataset[0].to(hp['device'])\n",
    "    input_dim=dataset.num_features\n",
    "    output_dim=dataset.num_classes\n",
    "    #model=GNNStack(input_dim,output_dim,hp,hp_appnp)  #APPNP\n",
    "    model=torch.nn.Linear(input_dim,output_dim)  #线性模型\n",
    "    \n",
    "    model.to(hp['device'])\n",
    "    #model.load_state_dict(best_models[i])  #APPNP\n",
    "    #y_soft=model(data)  #APPNP\n",
    "    y_soft=F.softmax(model(data.x),dim=1)  #线性模型\n",
    "    \n",
    "    total=0\n",
    "    correct=0\n",
    "    y=y_soft.max(dim=1)[1]\n",
    "    for mask in [data.test_mask]:\n",
    "        pred = y[mask]\n",
    "        label = data.y[mask]\n",
    "        total += torch.sum(mask).item()\n",
    "        correct += pred.eq(label).sum().item()\n",
    "    print('在数据集'+ds_rn_map[i][1]+'中，测试集上的原始accuracy是'+str(correct/total))\n",
    "    \n",
    "    print('Correct and smooth...')\n",
    "    \n",
    "    #y_soft=F.softmax(y_soft,dim=1)  #APPNP  #因为GNNStack用的是log softmax，所以不能直接用\n",
    "    mask=data.train_mask\n",
    "    label = data.y\n",
    "    y_soft = post.correct(y_soft,label[mask],mask,data.edge_index)\n",
    "    y_soft = post.smooth(y_soft,label[mask],mask,data.edge_index)\n",
    "    y_soft = y_soft.max(dim=1)[1]\n",
    "    \n",
    "    total=0\n",
    "    correct=0\n",
    "    for mask in [data.test_mask]:\n",
    "        pred = y_soft[mask]\n",
    "        label = data.y[mask]\n",
    "        total += torch.sum(mask).item()\n",
    "        correct += pred.eq(label).sum().item()\n",
    "        \n",
    "    print('在数据集'+ds_rn_map[i][1]+'中，测试集上的最终accuracy是'+str(correct/total))\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8505920",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
