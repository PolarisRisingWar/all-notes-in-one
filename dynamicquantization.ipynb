{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the modules used here in this recipe\n",
    "import torch\n",
    "import torch.quantization\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import os\n",
    "import time\n",
    "\n",
    "# define a very, very simple LSTM for demonstration purposes\n",
    "# in this case, we are wrapping nn.LSTM, one layer, no pre or post processing\n",
    "# inspired by\n",
    "# https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html, by Robert Guthrie\n",
    "# and https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html\n",
    "class lstm_for_demonstration(nn.Module):\n",
    "  \"\"\"Elementary Long Short Term Memory style model which simply wraps nn.LSTM\n",
    "     Not to be used for anything other than demonstration.\n",
    "  \"\"\"\n",
    "  def __init__(self,in_dim,out_dim,depth):\n",
    "     super(lstm_for_demonstration,self).__init__()\n",
    "     self.lstm = nn.LSTM(in_dim,out_dim,depth)\n",
    "\n",
    "  def forward(self,inputs,hidden):\n",
    "     out,hidden = self.lstm(inputs,hidden)\n",
    "     return out, hidden\n",
    "\n",
    "\n",
    "torch.manual_seed(29592)  # set the seed for reproducibility\n",
    "\n",
    "#shape parameters\n",
    "model_dimension=8\n",
    "sequence_length=20\n",
    "batch_size=1\n",
    "lstm_depth=1\n",
    "\n",
    "# random data for input\n",
    "inputs = torch.randn(sequence_length,batch_size,model_dimension)\n",
    "# hidden is actually is a tuple of the initial hidden state and the initial cell state\n",
    "hidden = (torch.randn(lstm_depth,batch_size,model_dimension), torch.randn(lstm_depth,batch_size,model_dimension))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Do the Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the floating point version of this module:\n",
      "lstm_for_demonstration(\n",
      "  (lstm): LSTM(8, 8)\n",
      ")\n",
      "\n",
      "and now the quantized version:\n",
      "lstm_for_demonstration(\n",
      "  (lstm): DynamicQuantizedLSTM(8, 8)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    " # here is our floating point instance\n",
    "float_lstm = lstm_for_demonstration(model_dimension, model_dimension,lstm_depth)\n",
    "\n",
    "# this is the call that does the work\n",
    "quantized_lstm = torch.quantization.quantize_dynamic(\n",
    "    float_lstm, {nn.LSTM, nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# show the changes that were made\n",
    "print('Here is the floating point version of this module:')\n",
    "print(float_lstm)\n",
    "print('')\n",
    "print('and now the quantized version:')\n",
    "print(quantized_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Look at Model Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  fp32  \t Size (KB): 4.051\n",
      "model:  int8  \t Size (KB): 2.963\n",
      "1.37 times smaller\n"
     ]
    }
   ],
   "source": [
    "#临时储存模型，计算储存空间，然后删除\n",
    "def print_size_of_model(model, label=\"\"):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size=os.path.getsize(\"temp.p\")\n",
    "    print(\"model: \",label,' \\t','Size (KB):', size/1e3)\n",
    "    os.remove('temp.p')\n",
    "    return size\n",
    "\n",
    "# compare the sizes\n",
    "f=print_size_of_model(float_lstm,\"fp32\")\n",
    "q=print_size_of_model(quantized_lstm,\"int8\")\n",
    "print(\"{0:.2f} times smaller\".format(f/q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Look at Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Floating point FP32\n",
      "830 µs ± 9.39 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(\"Floating point FP32\")\n",
    "%timeit float_lstm.forward(inputs, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized INT8\n",
      "913 µs ± 13.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(\"Quantized INT8\")\n",
    "%timeit quantized_lstm.forward(inputs,hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Look at Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean absolute value of output tensor values in the FP32 model is 0.12887 \n",
      "mean absolute value of output tensor values in the INT8 model is 0.12912\n",
      "mean absolute value of the difference between the output tensors is 0.00156 or 1.21 percent\n"
     ]
    }
   ],
   "source": [
    "# run the float model\n",
    "out1, hidden1 = float_lstm(inputs, hidden)\n",
    "mag1 = torch.mean(abs(out1)).item()\n",
    "print('mean absolute value of output tensor values in the FP32 model is {0:.5f} '.format(mag1))\n",
    "\n",
    "# run the quantized model\n",
    "out2, hidden2 = quantized_lstm(inputs, hidden)\n",
    "mag2 = torch.mean(abs(out2)).item()\n",
    "print('mean absolute value of output tensor values in the INT8 model is {0:.5f}'.format(mag2))\n",
    "\n",
    "# compare them\n",
    "mag3 = torch.mean(abs(out1-out2)).item()\n",
    "print('mean absolute value of the difference between the output tensors is {0:.5f} or {1:.2f} percent'.format(mag3,mag3/mag1*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f0503048227ae8776142d82c5d56af8c05f5d55a1bfc6e140eaeaf5efd5edce"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('envlegalai1': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
